import cv2
import torch
import pytesseract
import supervision as sv
from ultralytics import YOLO
import string
import pyttsx3  # For text-to-speech
import numpy as np

# Initialize text-to-speech engine
tts_engine = pyttsx3.init()

# Optional: Set Tesseract path if needed
# pytesseract.pytesseract.tesseract_cmd = r'<full_path_to_your_tesseract_executable>'

# Initialize model with proper weights loading
original_load = torch.load
torch.load = lambda *args, **kwargs: original_load(*args, **{**kwargs, 'weights_only': False})
model = YOLO('yolov10s-doclaynet.pt')

# Initialize webcam
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)  # Higher resolution for better small text capture
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)

box_annotator = sv.BoxAnnotator()
label_annotator = sv.LabelAnnotator()

# Cache last seen boxes to prevent OCR reprocessing
last_boxes = None
last_texts = []
box_keys = {}  # Dictionary to map keyboard keys to text boxes

# OCR preprocessing function for small text
def preprocess_for_ocr(image):
    # Convert to grayscale if not already
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image
    
    # Resize image (scale up small text)
    scale_factor = 2.0  # Adjust as needed
    resized = cv2.resize(gray, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_CUBIC)
    
    # Apply adaptive thresholding to deal with different lighting conditions
    thresh = cv2.adaptiveThreshold(resized, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                   cv2.THRESH_BINARY, 21, 15)
    
    # Noise removal
    kernel = np.ones((1, 1), np.uint8)
    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)
    
    # Edge enhancement
    edges = cv2.Canny(opening, 100, 200)
    dilated_edges = cv2.dilate(edges, kernel, iterations=1)
    
    # Combine the original threshold with edge enhancement
    processed = cv2.bitwise_and(opening, opening, mask=cv2.bitwise_not(dilated_edges))
    
    # Apply bilateral filter to smooth text while preserving edges
    processed = cv2.bilateralFilter(processed, 9, 75, 75)
    
    return processed

# Add zooming functionality for small text inspection
zoom_factor = 1.0
zoom_active = False
zoom_box_index = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break
        
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Run YOLO inference
    results = model(rgb_frame, imgsz=1024, conf=0.15, iou=0.7)[0]  # Lower confidence threshold for small text
    detections = sv.Detections.from_ultralytics(results)
    text_detections = sv.Detections(
        xyxy=detections.xyxy,
        confidence=detections.confidence,
        class_id=detections.class_id
    )
    
    # Check if boxes changed
    current_boxes = text_detections.xyxy if len(text_detections) > 0 else None
    boxes_changed = True
    if last_boxes is not None and current_boxes is not None:
        if last_boxes.shape == current_boxes.shape:
            boxes_changed = not (last_boxes == current_boxes).all()
    
    # Run OCR only if boxes changed
    if boxes_changed and current_boxes is not None:
        last_texts = []
        # Reset key mappings
        box_keys = {}
        
        # Assign alphabet keys to detected text boxes
        available_keys = list(string.ascii_lowercase)  # Use lowercase letters as keys
        
        for i, box in enumerate(current_boxes):
            x1, y1, x2, y2 = map(int, box)
            
            # Ensure box coordinates are within frame bounds
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)
            
            # Skip empty or invalid boxes
            if x2 <= x1 or y2 <= y1:
                last_texts.append("")
                continue
                
            roi = frame[y1:y2, x1:x2]
            
            # Preprocess the ROI for better OCR on small text
            processed_roi = preprocess_for_ocr(roi)
            
            # Fixed quotation issue in config string
            custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,;:!?()[]{}-_=+@#$%^&*<>/\\ --dpi 300'
            
            try:
                text = pytesseract.image_to_string(processed_roi, config=custom_config)
                
                # If text detection failed, try a different PSM mode
                if not text.strip():
                    custom_config = r'--oem 3 --psm 11 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,;:!?()[]{}-_=+@#$%^&*<>/\\ --dpi 300'
                    text = pytesseract.image_to_string(processed_roi, config=custom_config)
            except Exception as e:
                print(f"OCR error: {e}")
                # Fallback to basic OCR without custom config
                text = pytesseract.image_to_string(processed_roi)
            
            text = text.strip()
            last_texts.append(text)
            
            # Assign key if we have any left
            if i < len(available_keys):
                key = available_keys[i]
                box_keys[ord(key)] = {
                    'text': text,
                    'box': (x1, y1, x2, y2)
                }
        
        last_boxes = current_boxes
    
    # Create a copy for annotations
    display_frame = frame.copy()
    
    # Handle zoom view if active
    if zoom_active and current_boxes is not None and zoom_box_index < len(current_boxes):
        key_list = list(box_keys.keys())
        if zoom_box_index < len(key_list):
            current_key = key_list[zoom_box_index]
            box_info = box_keys[current_key]
            x1, y1, x2, y2 = box_info['box']
            
            # Extract and zoom the region
            roi = frame[y1:y2, x1:x2]
            if roi.size > 0:  # Check if ROI is valid
                # Calculate new dimensions while maintaining aspect ratio
                h, w = roi.shape[:2]
                zoom_w = int(w * zoom_factor)
                zoom_h = int(h * zoom_factor)
                
                # Resize the ROI
                zoomed_roi = cv2.resize(roi, (zoom_w, zoom_h), interpolation=cv2.INTER_CUBIC)
                
                # Create a blank canvas
                canvas = np.zeros_like(frame)
                
                # Center the zoomed ROI on the canvas
                y_offset = max(0, (frame.shape[0] - zoom_h) // 2)
                x_offset = max(0, (frame.shape[1] - zoom_w) // 2)
                
                # Place the zoomed ROI on the canvas
                canvas[y_offset:y_offset+zoom_h, x_offset:x_offset+zoom_w] = zoomed_roi
                
                # Add text overlay
                cv2.putText(
                    canvas,
                    f"ZOOMED VIEW: {chr(current_key)} - {box_info['text']}",
                    (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.7,
                    (0, 255, 0),
                    2
                )
                cv2.putText(
                    canvas,
                    "Press 'ESC' to exit zoom, '+'/'-' to adjust zoom level",
                    (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.7,
                    (0, 255, 0),
                    2
                )
                
                display_frame = canvas
    else:
        # Draw boxes
        if current_boxes is not None:
            display_frame = box_annotator.annotate(display_frame, text_detections)
        
        # Draw labels with key shortcuts
        if current_boxes is not None and len(last_texts) == len(current_boxes) and box_keys:
            # Create labels with key shortcuts
            labels = []
            for i, text in enumerate(last_texts):
                if i < len(string.ascii_lowercase):
                    key = string.ascii_lowercase[i]
                    # Add key prefix to the displayed label
                    short_text = text[:20] + "..." if len(text) > 20 else text
                    labels.append(f"[{key}] {short_text}")
                else:
                    labels.append(text[:20] + "..." if len(text) > 20 else text)
            
            display_frame = label_annotator.annotate(display_frame, text_detections, labels)
        
        # Display info text
        cv2.putText(
            display_frame,
            "Press letter key to read text. 'z' for zoom mode. 'q' to quit.",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (0, 255, 0),
            2
        )
    
    cv2.imshow("Small Text Reader", display_frame)
    
    # Check for key presses
    key = cv2.waitKey(1) & 0xFF
    
    # If the pressed key is in our mapping, read the text
    if key in box_keys:
        text_to_read = box_keys[key]['text']
        if text_to_read:
            print(f"Reading text: {text_to_read}")
            tts_engine.say(text_to_read)
            tts_engine.runAndWait()
        else:
            tts_engine.say("No text detected in this region")
            tts_engine.runAndWait()
    
    # Zoom mode toggle
    elif key == ord('z') and not zoom_active and box_keys:
        zoom_active = True
        zoom_box_index = 0
        zoom_factor = 3.0  # Initial zoom level
    
    # Exit zoom mode
    elif key == 27:  # ESC key
        zoom_active = False
    
    # Navigate through boxes in zoom mode
    elif zoom_active and key == ord('n') and box_keys:
        zoom_box_index = (zoom_box_index + 1) % len(box_keys)
    
    # Adjust zoom level
    elif zoom_active and key == ord('+'):
        zoom_factor = min(zoom_factor + 0.5, 10.0)
    elif zoom_active and key == ord('-'):
        zoom_factor = max(zoom_factor - 0.5, 1.0)
    
    # Quit on 'q'
    elif key == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
